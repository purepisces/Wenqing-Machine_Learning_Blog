# need 整理 这是gpt的
# 矩阵乘法的线性组合视角与维度条件

## 1️⃣ 关键等式

矩阵乘法可以写成线性组合形式：

$xE = ∑(i=1→n) x_i r_i$

其中：

- $x ∈ R^(1×n)$
- $E ∈ R^(n×m)$
- $r_i$ 是矩阵 E 的第 i 行

---

## 2️⃣ 成立的前提条件

必须满足：

x 的列数 = E 的行数

也就是：

$( m × n ) @ ( n × p ) → ( m × p )$

中间的 $n$ 必须相等，否则无法进行矩阵乘法。

---

## 3️⃣ 为什么必须相等？

矩阵乘法的定义是：

$y_j = ∑(i=1→n) x_i · E_(i,j)$

如果 $x$ 的维度和 $E$ 的维度对不上：

- $x$ 只有 3 个元素
- $E$ 却有 4 行

那么就无法一一对应相乘。

---

## 4️⃣ 线性组合视角

如果：

$x = [x1, x2, x3, x4] → (1,4)$

E 有 4 行：

E =
r1
r2
r3
r4

那么：

$xE = x1·r1 + x2·r2 + x3·r3 + x4·r4$

这说明：

矩阵乘法本质是对矩阵行向量的线性组合。

---

## 5️⃣ 更一般形式

如果：

$x ∈ R^(m×n)$
$E ∈ R^(n×p)$

则：

$xE ∈ R^(m×p)$

且第 k 行满足：

$(xE)_k = ∑(i=1→n) x_(k,i) r_i$

---

## 6️⃣ embedding lookup 特例

在 embedding 中：

one_hot.shape = (1, vocab_size)
E.shape = (vocab_size, hidden_dim)

因为：

vocab_size = vocab_size

所以可以写成：

$xE = ∑(i=1→vocab_size) x_i r_i$

当 x 是 one-hot 向量时：

x = [0,0,1,0,...]

则：

xE = 第 3 行

也就是说：

embedding lookup 本质是矩阵行选择。

---

## 🎯 最终总结

矩阵乘法：

(1×n) @ (n×m)

等价于：

xE = ∑ x_i E_(i,*)

前提条件：

x 的第二维 = E 的第一维

本质：

矩阵乘法就是线性组合行向量。
