
1.  **Batch Size (`batch_size`)** → Number of sentences processed in parallel.
2.  **Sequence Length (`sequence_length`)** → Number of tokens per sentence.
3.  **Padding is used** to ensure all sequences in a batch have the same length.
4.  **Transformers process tokens in parallel**, making these dimensions essential.
